{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-02T17:08:55.451591Z",
     "start_time": "2025-05-02T17:08:54.817747Z"
    }
   },
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))))\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Modelling_1\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T17:08:55.963431Z",
     "start_time": "2025-05-02T17:08:55.458092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from preprocessing.preprocessing import preprocess_data\n",
    "\n",
    "df_train, df_test = preprocess_data()\n",
    "\n",
    "print(\"Training data sample:\")\n",
    "df_train.select(\"features\", \"Accident_Severity\").show(5, truncate=False)"
   ],
   "id": "347bac31d39a54d0",
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Users/m4/Desktop/school/4/TSVD/assignment/Big-Data-Processing/data.tmp/CarAccidents/Accidents.csv.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAnalysisException\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpreprocessing\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpreprocessing\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m preprocess_data\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m df_train, df_test = \u001B[43mpreprocess_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mTraining data sample:\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      6\u001B[39m df_train.select(\u001B[33m\"\u001B[39m\u001B[33mfeatures\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mAccident_Severity\u001B[39m\u001B[33m\"\u001B[39m).show(\u001B[32m5\u001B[39m, truncate=\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/school/4/TSVD/assignment/Big-Data-Processing/preprocessing/preprocessing.py:39\u001B[39m, in \u001B[36mpreprocess_data\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     32\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     33\u001B[39m \u001B[33;03mMain preprocessing function that processes the data and returns training and test datasets\u001B[39;00m\n\u001B[32m     34\u001B[39m \u001B[33;03m\u001B[39;00m\n\u001B[32m     35\u001B[39m \u001B[33;03mReturns:\u001B[39;00m\n\u001B[32m     36\u001B[39m \u001B[33;03m    Tuple of (df_train_model, df_test_model) containing the preprocessed dataframes\u001B[39;00m\n\u001B[32m     37\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     38\u001B[39m \u001B[38;5;66;03m# Load and integrate data\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m df_train, df_test = \u001B[43mintegration\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     41\u001B[39m \u001B[38;5;66;03m# Remove unnecessary columns\u001B[39;00m\n\u001B[32m     42\u001B[39m df_train = df_train.drop(\u001B[33m\"\u001B[39m\u001B[33mAccident_Index\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mDate\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mTime\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mLongitude\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mLatitude\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mLSOA_of_Accident_Location\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/school/4/TSVD/assignment/Big-Data-Processing/integration/integration.py:9\u001B[39m, in \u001B[36mintegration\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m      6\u001B[39m spark.sparkContext.setLogLevel(\u001B[33m\"\u001B[39m\u001B[33mERROR\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# Načítanie dát\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m df_accidents = \u001B[43mspark\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m../data.tmp/CarAccidents/Accidents.csv\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheader\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minferSchema\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     10\u001B[39m df_casualties = spark.read.csv(\u001B[33m\"\u001B[39m\u001B[33m../data.tmp/CarAccidents/Casualties.csv\u001B[39m\u001B[33m\"\u001B[39m, header=\u001B[38;5;28;01mTrue\u001B[39;00m, inferSchema=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     11\u001B[39m df_vehicles = spark.read.csv(\u001B[33m\"\u001B[39m\u001B[33m../data.tmp/CarAccidents/Vehicles.csv\u001B[39m\u001B[33m\"\u001B[39m, header=\u001B[38;5;28;01mTrue\u001B[39;00m, inferSchema=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/tsvd/lib/python3.13/site-packages/pyspark/sql/readwriter.py:740\u001B[39m, in \u001B[36mDataFrameReader.csv\u001B[39m\u001B[34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[39m\n\u001B[32m    738\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) == \u001B[38;5;28mlist\u001B[39m:\n\u001B[32m    739\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m._spark._sc._jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m740\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jreader\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_spark\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_sc\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_jvm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mPythonUtils\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtoSeq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    741\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, RDD):\n\u001B[32m    743\u001B[39m     \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfunc\u001B[39m(iterator):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/tsvd/lib/python3.13/site-packages/py4j/java_gateway.py:1322\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1316\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1317\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1318\u001B[39m     args_command +\\\n\u001B[32m   1319\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1321\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1322\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1323\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1325\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1326\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/tsvd/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:185\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    181\u001B[39m converted = convert_exception(e.java_exception)\n\u001B[32m    182\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[32m    183\u001B[39m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[32m    184\u001B[39m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m185\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    186\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    187\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[31mAnalysisException\u001B[39m: [PATH_NOT_FOUND] Path does not exist: file:/Users/m4/Desktop/school/4/TSVD/assignment/Big-Data-Processing/data.tmp/CarAccidents/Accidents.csv."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"Accident_Severity\", outputCol=\"label\")\n",
    "df_train = labelIndexer.fit(df_train).transform(df_train)\n",
    "df_test = labelIndexer.fit(df_test).transform(df_test)"
   ],
   "id": "4c8af1d9ee995367",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, data, model_name):\n",
    "    \"\"\"Evaluate model performance with confusion matrix and metrics\"\"\"\n",
    "    # Make predictions\n",
    "    predictions = model.transform(data)\n",
    "\n",
    "    # Select (prediction, label) for confusion matrix\n",
    "    predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "    # Print contingency table (confusion matrix)\n",
    "    labels = metrics.labels\n",
    "    print(f\"\\n--- {model_name} Confusion Matrix ---\")\n",
    "\n",
    "    # Header row for confusion matrix\n",
    "    header = \"True \\\\ Predicted\"\n",
    "    for l in labels:\n",
    "        header += f\"\\t{int(l)}\"\n",
    "    print(header)\n",
    "\n",
    "    # Print each row of confusion matrix\n",
    "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "    for i, l in enumerate(labels):\n",
    "        row = f\"{int(l)}\"\n",
    "        for j in range(len(labels)):\n",
    "            row += f\"\\t{int(confusion_matrix[i][j])}\"\n",
    "        print(row)\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = metrics.precision()\n",
    "    recall = metrics.recall()\n",
    "    f1Score = metrics.fMeasure()\n",
    "\n",
    "    # Calculate MCC\n",
    "    # For binary classification, we can calculate MCC as follows\n",
    "    if len(labels) == 2:\n",
    "        TP = metrics.truePositiveRate(1.0) * confusion_matrix[1][1]\n",
    "        TN = metrics.truePositiveRate(0.0) * confusion_matrix[0][0]\n",
    "        FP = metrics.falsePositiveRate(0.0) * confusion_matrix[0][0]\n",
    "        FN = metrics.falsePositiveRate(1.0) * confusion_matrix[1][1]\n",
    "\n",
    "        denominator = np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "        mcc = ((TP * TN) - (FP * FN)) / denominator if denominator != 0 else 0\n",
    "    else:\n",
    "        # For multiclass MCC, we'll use a generic implementation\n",
    "        n_classes = len(labels)\n",
    "        confusion_sum = confusion_matrix.sum()\n",
    "\n",
    "        # Initialize values\n",
    "        cov_xy = 0\n",
    "        cov_xx = 0\n",
    "        cov_yy = 0\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            row_sum = sum(confusion_matrix[i])\n",
    "            col_sum = sum(confusion_matrix[:, i])\n",
    "\n",
    "            cov_xy += row_sum * col_sum\n",
    "            cov_xx += row_sum * row_sum\n",
    "            cov_yy += col_sum * col_sum\n",
    "\n",
    "        mcc_numerator = 0\n",
    "        for i in range(n_classes):\n",
    "            for j in range(n_classes):\n",
    "                mcc_numerator += confusion_matrix[i][j] * (confusion_matrix.sum() * confusion_matrix[i][j] -\n",
    "                                                         sum(confusion_matrix[i]) * sum(confusion_matrix[:, j]))\n",
    "\n",
    "        mcc_denominator = np.sqrt((confusion_sum**2 - cov_xx) * (confusion_sum**2 - cov_yy))\n",
    "        mcc = mcc_numerator / mcc_denominator if mcc_denominator != 0 else 0\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\n--- {model_name} Evaluation Metrics ---\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1Score:.4f}\")\n",
    "    print(f\"MCC: {mcc:.4f}\\n\")\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1Score, \"mcc\": mcc}"
   ],
   "id": "4168b4fb82633453",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Decision Tree Model\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=5)\n",
    "dt_model = dt.fit(df_train)\n",
    "\n",
    "# Evaluate Decision Tree model\n",
    "dt_metrics = evaluate_model(dt_model, df_test, \"Decision Tree\")\n",
    "\n",
    "# Print feature importances for Decision Tree if available\n",
    "if hasattr(dt_model, \"featureImportances\"):\n",
    "    importances = dt_model.featureImportances\n",
    "    print(\"Feature Importances:\")\n",
    "    for i, importance in enumerate(importances):\n",
    "        print(f\"Feature {i}: {importance}\")\n",
    "\n",
    "# 2. Linear SVM Model\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1, labelCol=\"label\", featuresCol=\"features\")\n",
    "lsvc_model = lsvc.fit(df_train)\n",
    "\n",
    "# Evaluate Linear SVM model\n",
    "lsvc_metrics = evaluate_model(lsvc_model, df_test, \"Linear SVM\")"
   ],
   "id": "65b656475399183a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compare models\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"Model\\t\\tPrecision\\tRecall\\t\\tF1 Score\\tMCC\")\n",
    "print(f\"Decision Tree\\t{dt_metrics['precision']:.4f}\\t\\t{dt_metrics['recall']:.4f}\\t\\t{dt_metrics['f1']:.4f}\\t\\t{dt_metrics['mcc']:.4f}\")\n",
    "print(f\"Linear SVM\\t{lsvc_metrics['precision']:.4f}\\t\\t{lsvc_metrics['recall']:.4f}\\t\\t{lsvc_metrics['f1']:.4f}\\t\\t{lsvc_metrics['mcc']:.4f}\")\n",
    "\n",
    "# Save models\n",
    "models_dir = os.path.join(\"..\", \"models\")\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "dt_model_path = os.path.join(models_dir, \"decision_tree_model\")\n",
    "dt_model.save(dt_model_path)\n",
    "print(f\"Decision Tree model saved to {dt_model_path}\")\n",
    "\n",
    "lsvc_model_path = os.path.join(models_dir, \"linear_svm_model\")\n",
    "lsvc_model.save(lsvc_model_path)\n",
    "print(f\"Linear SVM model saved to {lsvc_model_path}\")\n",
    "\n",
    "# Plot confusion matrices if matplotlib is available\n",
    "try:\n",
    "    # Plot Decision Tree confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(dt_metrics[\"confusion_matrix\"], interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Decision Tree Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(models_dir, \"dt_confusion_matrix.png\"))\n",
    "\n",
    "    # Plot Linear SVM confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(lsvc_metrics[\"confusion_matrix\"], interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Linear SVM Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(models_dir, \"svm_confusion_matrix.png\"))\n",
    "except Exception as e:\n",
    "    print(f\"Couldn't plot confusion matrices: {e}\")"
   ],
   "id": "58cc08673fe5994e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
